{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f09fe3",
   "metadata": {},
   "source": [
    "# numerical differentiation examples\n",
    "\n",
    "demonstration of finite difference methods and Richardson extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a28cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numeric_integrator import differentiate\n",
    "\n",
    "print(\"library loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15e6d7",
   "metadata": {},
   "source": [
    "## example 1: basic differentiation\n",
    "\n",
    "compute $\\frac{d}{dx}\\sin(x)$ at $x=\\pi/4$, exact: $\\cos(\\pi/4) = \\frac{\\sqrt{2}}{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1698795",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.sin\n",
    "x0 = np.pi / 4\n",
    "exact = np.cos(x0)\n",
    "\n",
    "# compare methods\n",
    "methods = [\"forward\", \"backward\", \"central\", \"richardson\"]\n",
    "h = 0.001\n",
    "\n",
    "print(f\"computing derivative at x={x0:.4f}\\n\")\n",
    "for method in methods:\n",
    "    result = differentiate(f, x0, method=method, h=h)\n",
    "    error = abs(result.value - exact)\n",
    "    print(f\"{method:10s}: value={result.value:.10f}, error={error:.2e}\")\n",
    "\n",
    "print(f\"\\nexact:      {exact:.10f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d901f",
   "metadata": {},
   "source": [
    "## example 2: step size analysis\n",
    "\n",
    "study how error varies with step size h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd8da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3\n",
    "x0 = 2.0\n",
    "exact = 3 * x0**2  # derivative of x^3 is 3x^2\n",
    "\n",
    "h_values = np.logspace(-8, -1, 30)\n",
    "errors_forward = []\n",
    "errors_central = []\n",
    "errors_richardson = []\n",
    "\n",
    "for h in h_values:\n",
    "    result_fwd = differentiate(f, x0, method=\"forward\", h=h)\n",
    "    result_cen = differentiate(f, x0, method=\"central\", h=h)\n",
    "    result_ric = differentiate(f, x0, method=\"richardson\", h=h, n=3)\n",
    "    \n",
    "    errors_forward.append(abs(result_fwd.value - exact))\n",
    "    errors_central.append(abs(result_cen.value - exact))\n",
    "    errors_richardson.append(abs(result_ric.value - exact))\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(h_values, errors_forward, 'o-', label='forward O(h)', linewidth=2)\n",
    "plt.loglog(h_values, errors_central, 's-', label='central O(h²)', linewidth=2)\n",
    "plt.loglog(h_values, errors_richardson, '^-', label='richardson', linewidth=2)\n",
    "plt.loglog(h_values, h_values, '--', label='O(h)', alpha=0.3)\n",
    "plt.loglog(h_values, h_values**2, '--', label='O(h²)', alpha=0.3)\n",
    "plt.xlabel('step size (h)', fontsize=12)\n",
    "plt.ylabel('absolute error', fontsize=12)\n",
    "plt.title('differentiation error vs step size', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"note: for very small h, roundoff error dominates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706183d4",
   "metadata": {},
   "source": [
    "## example 3: Richardson extrapolation demonstration\n",
    "\n",
    "show how Richardson improves accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c728001",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: np.exp(x) * np.sin(x)\n",
    "x0 = 1.0\n",
    "exact = np.exp(x0) * (np.sin(x0) + np.cos(x0))\n",
    "\n",
    "h = 0.1\n",
    "print(f\"computing d/dx[exp(x)sin(x)] at x={x0}\\n\")\n",
    "\n",
    "# different Richardson orders\n",
    "for n in [1, 2, 3, 4, 5]:\n",
    "    result = differentiate(f, x0, method=\"richardson\", h=h, n=n)\n",
    "    error = abs(result.value - exact)\n",
    "    print(f\"richardson n={n}: value={result.value:.12f}, error={error:.2e}, evals={result.n_evaluations}\")\n",
    "\n",
    "print(f\"\\nexact:          {exact:.12f}\")\n",
    "print(f\"\\nhigher n improves accuracy but requires more evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e84d6b",
   "metadata": {},
   "source": [
    "## example 4: second derivative\n",
    "\n",
    "compute $\\frac{d^2}{dx^2}\\sin(x) = -\\sin(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de98bac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numeric_integrator.differentiators import second_derivative\n",
    "\n",
    "f = np.sin\n",
    "x_values = np.linspace(0, 2*np.pi, 10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# compute second derivatives\n",
    "second_derivs = []\n",
    "for x in x_values:\n",
    "    result = second_derivative(f, x, h=0.001)\n",
    "    second_derivs.append(result.value)\n",
    "\n",
    "# plot\n",
    "x_fine = np.linspace(0, 2*np.pi, 200)\n",
    "plt.plot(x_fine, np.sin(x_fine), 'b-', linewidth=2, label='f(x) = sin(x)', alpha=0.7)\n",
    "plt.plot(x_fine, -np.sin(x_fine), 'r-', linewidth=2, label=\"f''(x) = -sin(x) (exact)\", alpha=0.7)\n",
    "plt.plot(x_values, second_derivs, 'ro', markersize=8, label=\"f''(x) (numerical)\")\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('second derivative verification', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# compute errors\n",
    "errors = [abs(num + np.sin(x)) for num, x in zip(second_derivs, x_values)]\n",
    "print(f\"average error: {np.mean(errors):.2e}\")\n",
    "print(f\"max error:     {np.max(errors):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01df4aa8",
   "metadata": {},
   "source": [
    "## example 5: gradient computation\n",
    "\n",
    "compute gradient of multivariable function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43a688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numeric_integrator.differentiators import derivative_vector\n",
    "\n",
    "# function: f(x,y) = x^2 + y^2\n",
    "# gradient: [2x, 2y]\n",
    "def f(v):\n",
    "    x, y = v\n",
    "    return x**2 + y**2\n",
    "\n",
    "# compute gradient at several points\n",
    "points = [(1, 1), (2, 0), (0, 3), (-1, 2)]\n",
    "\n",
    "print(\"gradient of f(x,y) = x² + y²\\n\")\n",
    "for point in points:\n",
    "    result = derivative_vector(f, np.array(point), h=0.001)\n",
    "    exact_grad = 2 * np.array(point)\n",
    "    error = np.linalg.norm(result.value - exact_grad)\n",
    "    print(f\"point {point}:\")\n",
    "    print(f\"  numerical: [{result.value[0]:.6f}, {result.value[1]:.6f}]\")\n",
    "    print(f\"  exact:     [{exact_grad[0]:.6f}, {exact_grad[1]:.6f}]\")\n",
    "    print(f\"  error:     {error:.2e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f143e3bf",
   "metadata": {},
   "source": [
    "## example 6: optimal step size\n",
    "\n",
    "find step size that balances truncation and roundoff error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696d51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numeric_integrator.utils import optimal_step_size\n",
    "\n",
    "f = lambda x: np.exp(x)\n",
    "x0 = 1.0\n",
    "\n",
    "# optimal step for different orders\n",
    "print(\"optimal step sizes:\\n\")\n",
    "for order in [1, 2, 4]:\n",
    "    h_opt = optimal_step_size(f, x0, order=order)\n",
    "    print(f\"order {order}: h_opt = {h_opt:.2e}\")\n",
    "    \n",
    "    # test with optimal h\n",
    "    if order == 1:\n",
    "        result = differentiate(f, x0, method=\"forward\", h=h_opt)\n",
    "    else:\n",
    "        result = differentiate(f, x0, method=\"central\", h=h_opt)\n",
    "    \n",
    "    exact = np.exp(x0)\n",
    "    error = abs(result.value - exact)\n",
    "    print(f\"  error with h_opt: {error:.2e}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
